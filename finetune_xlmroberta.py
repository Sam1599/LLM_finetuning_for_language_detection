# -*- coding: utf-8 -*-
"""Finetune_xlmroberta.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1thWZYcNnbcmd2aZz23qUkv2MpVMoqyMm
"""

! pip install datasets
! pip install -U accelerate
! pip install -U transformers

"""Import the necessary libraries"""

from transformers import pipeline
import pandas as pd
from transformers import XLMRobertaForSequenceClassification, XLMRobertaTokenizer, Trainer, TrainingArguments
from datasets import load_dataset, Features

languages = ['hi', 'asm', 'tam', 'ur']

"""Loading the pretrained model for language detection"""

model_ckpt = "papluca/xlm-roberta-base-language-detection"
pipe = pipeline("text-classification", model=model_ckpt)

text = [
    "aaj ka mausam achchha hai",  # Hindi
    "aap kaise ho",               # Hindi
    "Nee epdi iruka",             # Tamil
    "nalla kaalam und",           # Malayalam
    "nalla kalam undi",          # Telugu
    "Na rakh Umeed-e-Wafa Parinday Se" #Urdu
]

results = pipe(text, top_k=1, truncation=True)

"""Detects the language and prints the result"""

for t, r in zip(text, results):
    lang = r[0]['label']
    if lang in languages:
        print(f"{t} is in {lang}")

"""The model works for the Indian language- Hindi. The model is fine tuned further to detect other languages like Tamil, Malayalam,etc.

The Aksharantar dataset from hugging face is loaded in the appropriate format
"""

from datasets import load_dataset, Features, Value

features = Features({
    'unique_identifier': Value('string'),
    'native word': Value('string'),
    'english word': Value('string'),
    'source': Value('string'),
    'score': Value('double')
})

dataset = load_dataset('ai4bharat/Aksharantar', features=features, split="train")

dataset.features

dataset = dataset.filter(lambda x: x['unique_identifier'].startswith(('hin', 'asm', 'tam', 'urd')))

dataset = dataset.train_test_split(train_size=0.001, shuffle=True)

dataset = dataset['train']

dataset = dataset.map(lambda x: {'lang': x['unique_identifier'][:3]}, remove_columns=['source'])

languages = dataset.unique('lang')
print(languages)

dataset[0]

dataset[1]

dataset[40]

dataset[50]

"""The data is tokenized using the pretrained tokenizer of the xlm-roberta model"""

tokenizer = XLMRobertaTokenizer.from_pretrained("xlm-roberta-base")

language_to_id = {lang: idx for idx, lang in enumerate(languages)}
id_to_language = {idx: lang for idx, lang in enumerate(languages)}

def tokenize_function(example):
  inputs = tokenizer(example['english word'], padding='max_length', truncation=True)
  inputs['labels'] = [language_to_id[source] for source in example['lang']]
  return inputs

"""Applying the tokenize function to each sample in the dataset in batches"""

tokenized_dataset = dataset.map(tokenize_function, batched=True, num_proc= 8)

tokenized_dataset = tokenized_dataset.train_test_split(test_size=0.1)
train_dataset= tokenized_dataset['train']
val_dataset= tokenized_dataset['test']

model = XLMRobertaForSequenceClassification.from_pretrained("xlm-roberta-base", num_labels=len(languages), id2label=id_to_language, label2id=language_to_id)

training_args = TrainingArguments(
    output_dir="fine_tuned_model",
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    learning_rate=2e-5,
    #weight_decay=0.01,
    evaluation_strategy="epoch",
    num_train_epochs=3,
    logging_dir="./logs",
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset= tokenized_dataset['train'],
    eval_dataset= tokenized_dataset['test'],
)

trainer.train()

results = trainer.evaluate()
print(results)

pipe = pipeline("text-classification", model=model, tokenizer=tokenizer)

text = [
    "aaj ka mausam achchha hai",        # Hindi
    "kafi weqt se ap ko dekha nehin",   # Urdu
    'aponac log pai bhal lagil',        # Assamese
    "neram nalla iruku",                # Tamil

]

results = pipe(text, truncation=True)

results

for t, r in zip(text, results):
    lang = r['label']
    if lang in languages:
        print(f"{t} is in {lang.upper()}")

